---
title: CS224W-2-传统的图机器学习
tags: [机器学习, 深度学习, 基本概念]
mathjax: true
toc: true
date: 2022-07-19 09:42:59
categories:
    - [算法]
    - [学习]
---
这是CS224W课程的第二篇，主要内容是传统的图机器学习方法
<!-- more -->

## 前言

回顾图机器学习的主要任务包括以下几个部分：节点层级的预测、链接层级的预测、图层级的预测。在传统的流程中，主要是从上述层级中设计特征后再从训练数据中获得特征，这也是遵循了传统机器学习的路子：训练模型-应用模型。从中可以看出，需要构建出有效的特征来达到较好的模型性能，而传统的机器学习使用的是手工设计的特征来实现节点、链接、图层级的预测，为了尽可能说明问题，这里主要集中在无向图中。
图机器学习的目标是依据一组对象的集合进行预测。为此，可以做出以下假设，对于特征，可以定义为一个$d$维的向量；对于节点、边、节点集合、边集合都可以定义为对象；那有了这样的定义后，那对于节点层级的预测问题可以形式化表示为：对于给定的$G=(V,E)$，需要学习一个函数$f:V\rightarrow \mathbb{R}$可以根据给定的节点结合$V$找到关联关系，那如何学习这个关系呢？下面分别从不同的层级来进行介绍。

## 节点层级

目标是实现表征网络中节点结构和位置，例如：节点度数、节点中心性、聚类系数、非同构子图单元（Graphlet）等。
其中节点$v$的度数$k_v$是指这个节点所包含的边数，这个时候并未考虑节点的重要性，而节点$v$的中心性$c_v$则进一步考虑了这点。

### 特征向量中心性

这个中心性定义认为一个重要的节点$v$周边应当环绕着一些重要的节点$u\in N(v)$，那节点的中心性则可以用周边节点的中心性平均值来度量，即
$$c_v=\frac{1}{\lambda}\sum_{u\in N(v)}c_u,$$其中$\lambda$是归一化常数，一般是邻接矩阵的最大的特征值。注意到这个问题实际上是一个递归问题，即计算当前节点之前要计算周边节点，这样的形式实际上是无解的。但换个写法表示的话即可变为$$\lambda c=Ac,$$其中$A$是邻接矩阵（$A_{uv}=1$，如果$u\in N(v)$），$c$是所有节点的中心性向量，$\lambda$是特征值，依据Perron-Frobenius定理，$\lambda_{max}$总是正值且唯一，那就可以相应的计算出对应的$c_{max}$^[这里提到的变换，我没看懂，证明的过程还没找到依据，这个定理我也不清楚。]

### 介数中心性

这个中心性认为一个节点如果在其他节点最短路径之间出现次数越多越重要。可以表示为$$c_v=\sum_{s\neq v\neq t}\frac{s和t最短路径中包含v的数量}{s和t最短路径数}$$例如：
![介数中心性](https://raw.githubusercontent.com/Waynehfut/blog/img/img/20220719172817.png)

### 接近中心性

这个中心性认为一个节点如果到其他节点的路径越短越重要，形式化表示为$$c_v=\frac{1}{\sum_{u\neq v}u和v之间的最短距离}$$例如：
![接近中心性](https://raw.githubusercontent.com/Waynehfut/blog/img/img/20220719173441.png)

## 聚类系数

这个主要是来度量$v$的节点相邻情况，即这个点的相邻点之间有多少是两两互通的比值^[参考维基百科 <https://zh.m.wikipedia.org/zh-cn/%E9%9B%86%E8%81%9A%E7%B3%BB%E6%95%B0>]，可以用如下公式表示：$$e_v=\frac{相邻节点之间的边数}{相邻节点组合数}=\frac{相邻节点边数}{C_{k_v}^2}$$
例如：
![相邻节点](https://raw.githubusercontent.com/Waynehfut/blog/img/img/20220719203356.png)

## Graphlet

从聚类系数开始往后，仔细观察可以发现，实际上相邻节点边数就是包含目标节点的三角形的数量，例如上图的中间部分，这样的三元组被称之为Graphlet（当然还有其他形式）。
![Graphlet](https://raw.githubusercontent.com/Waynehfut/blog/img/img/20220719211203.png)
这个词，我实在是没找到合适的翻译来对应，有的人翻译为图基元，有的翻译为图元。更多是直接用Graphlet，意思就是图的核心组成部件。这个定义的目的在于描述节点$u$周边的图网络结构情况。在此基础上，学者定义了2-5个节点的所有异构子图，共有73个模板，例如右上角的图就有0,1,2,3,5,10,11,...等情况。
![](https://raw.githubusercontent.com/Waynehfut/blog/img/img/20220719212530.png)